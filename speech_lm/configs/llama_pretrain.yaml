paths:
  run_dir: results/pretrain
  checkpoint_dir: ${paths.run_dir}/checkpoints

hydra:
  run:
    dir: ${paths.run_dir}

trainer:
  _target_: lightning.fabric.Fabric
  accelerator: gpu
  strategy:
    _target_: lightning.fabric.strategies.DDPStrategy
    static_graph: true
  num_nodes: 4
  devices: 8
  precision: bf16-mixed
  loggers:
    _target_: pytorch_lightning.loggers.TensorBoardLogger
    save_dir: ${paths.run_dir}
    name: tensorboard
    version: null

model:
  _target_: transformers.AutoModelForCausalLM.from_pretrained
  pretrained_model_name_or_path: fishaudio/speech-lm-300m
  revision: init

tokenizer:
  _target_: transformers.AutoTokenizer.from_pretrained
  pretrained_model_name_or_path: fishaudio/speech-lm-300m
  revision: init

# Say we want a 3 trillion seen token schedule
# 3e12 / 1024 / 512 / 8 = 715255
# But we use a 100k steps schedule here to save time
# This is a 300 billion seen token schedule
schedule:
  max_length: 1024
  batch_size: 128  # 128 * 4 = 512
  micro_batch_size: 8
  max_steps: 100000
  save_interval: 5000
  log_interval: 10
  gradient_accumulation_steps: "${eval: ${schedule.batch_size} // ${schedule.micro_batch_size}}"
  clip_grad_norm: 1.0

dataset:
  _target_: speech_lm.datasets.cultura_x.InterleaveDataset
  datasets:
    - _target_: speech_lm.datasets.cultura_x.CulturaXDataset
      lang: 'en'
    - _target_: speech_lm.datasets.cultura_x.CulturaXDataset
      lang: 'zh'
    - _target_: speech_lm.datasets.cultura_x.CulturaXDataset
      lang: 'ja'
  probabilities: [0.4, 0.3, 0.3]
  seed: 42

train_dataloader:
  _target_: torch.utils.data.DataLoader
  dataset: ${dataset}
  batch_size: ${schedule.micro_batch_size}
  num_workers: 8
  collate_fn:
    _target_: speech_lm.datasets.cultura_x.CulutreXCollator
    tokenizer: ${tokenizer}
    max_length: ${schedule.max_length}

optimizer:
  _target_: torch.optim.AdamW
  lr: 3e-4
  weight_decay: 0.1
  betas: [0.9, 0.95]
  eps: 1e-5

scheduler:
  _target_: torch.optim.lr_scheduler.LambdaLR
  lr_lambda:
    _target_: speech_lm.scheduler.get_cosine_schedule_with_warmup_lr_lambda
    _partial_: true
    num_warmup_steps: 2000
    num_training_steps: ${schedule.max_steps}
    final_lr_ratio: 0.1
