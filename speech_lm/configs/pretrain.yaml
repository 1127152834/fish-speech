paths:
  run_dir: results/pretrain
  checkpoint_dir: ${paths.run_dir}/checkpoints

hydra:
  run:
    dir: ${paths.run_dir}

trainer:
  _target_: lightning.fabric.Fabric
  accelerator: gpu
  strategy:
    _target_: lightning.fabric.strategies.FSDPStrategy
    sync_module_states: true
    use_orig_params: true
    cpu_offload: false
    mixed_precision:
      _target_: torch.distributed.fsdp.MixedPrecision
      param_dtype: 
        _target_: hydra.utils.get_object
        path: torch.bfloat16
      reduce_dtype:
        _target_: hydra.utils.get_object
        path: torch.bfloat16
      buffer_dtype:
        _target_: hydra.utils.get_object
        path: torch.bfloat16
      cast_forward_inputs: true
    sharding_strategy: SHARD_GRAD_OP
    auto_wrap_policy:
      _target_: torch.distributed.fsdp.wrap.transformer_auto_wrap_policy
      _partial_: true
      transformer_layer_cls:
        - _target_: hydra.utils.get_class
          path: transformers.models.llama.modeling_llama.LlamaDecoderLayer
    activation_checkpointing_policy: ${trainer.strategy.auto_wrap_policy}
    state_dict_type: full
  num_nodes: 1
  devices: 8
  precision: bf16-mixed
  loggers:
    _target_: pytorch_lightning.loggers.TensorBoardLogger
    save_dir: ${paths.run_dir}
    name: tensorboard
    version: null

model:
  _target_: transformers.AutoModelForCausalLM.from_pretrained
  pretrained_model_name_or_path: fishaudio/speech-lm-300m
  revision: init

tokenizer:
  _target_: transformers.AutoTokenizer.from_pretrained
  pretrained_model_name_or_path: fishaudio/speech-lm-300m
  revision: init

# Say we want a 3 trillion seen token schedule
# 3e12 / 1024 / 512 / 8 = 715255
schedule:
  max_length: 1024
  batch_size: 512  # 128 * 4 = 512
  micro_batch_size: 64
  max_steps: 715255
  save_interval: 2000
  log_interval: 10
  gradient_accumulation_steps: "${eval: ${schedule.batch_size} // ${schedule.micro_batch_size}}"
  clip_grad_norm: 1.0

dataset:
  _target_: speech_lm.datasets.cultura_x.InterleaveDataset
  datasets:
    - _target_: speech_lm.datasets.cultura_x.CulturaXDataset
      lang: 'en'
    - _target_: speech_lm.datasets.cultura_x.CulturaXDataset
      lang: 'zh'
    - _target_: speech_lm.datasets.cultura_x.CulturaXDataset
      lang: 'ja'
  probabilities: [0.4, 0.3, 0.3]
  seed: 42

dataloader:
  _target_: torch.utils.data.DataLoader
  dataset: ${dataset}
  batch_size: ${schedule.micro_batch_size}
  num_workers: 4
  collate_fn:
    _target_: speech_lm.datasets.cultura_x.CulutreXCollator
    tokenizer: ${tokenizer}
    max_length: ${schedule.max_length}

optimizer:
  _target_: torch.optim.AdamW
  lr: 3e-4
  weight_decay: 0.1
  betas: [0.9, 0.95]
  eps: 1e-5

scheduler:
  _target_: torch.optim.lr_scheduler.LambdaLR
  lr_lambda:
    _target_: speech_lm.scheduler.get_cosine_schedule_with_warmup_lr_lambda
    _partial_: true
    num_warmup_steps: 2000
    num_training_steps: ${schedule.max_steps}
    final_lr_ratio: 0.1
