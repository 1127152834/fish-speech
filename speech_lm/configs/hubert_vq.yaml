paths:
  run_dir: results/hubert-vq
  checkpoint_dir: ${paths.run_dir}/checkpoints

hydra:
  run:
    dir: ${paths.run_dir}

trainer:
  _target_: lightning.fabric.Fabric
  accelerator: gpu
  strategy: 
    _target_: lightning.fabric.strategies.DDPStrategy
    find_unused_parameters: true
    static_graph: true

  devices: auto
  precision: bf16-mixed
  loggers:
    _target_: pytorch_lightning.loggers.TensorBoardLogger
    save_dir: ${paths.run_dir}
    name: tensorboard
    version: null

model:
  _target_: speech_lm.models.hubert_vq.HubertVQDistill
  model_name_or_path: facebook/hubert-large-ls960-ft
  vq_layer: -4
  codebook_size: 4096
  trainable_layers_before_vq: 2
  trainable_layers_after_vq: 2
  vq_loss_weight: 1.0

schedule:
  batch_size: 32
  micro_batch_size: 32
  max_steps: 10000
  save_interval: 2000
  gradient_accumulation_steps: "${eval: ${schedule.batch_size} // ${schedule.micro_batch_size}}"
  clip_grad_norm: 1.0

dataset:
  _target_: speech_lm.datasets.hubert_vq.HubertVQDataset
  filelist: libritts-r.filelist

dataloader:
  _target_: torch.utils.data.DataLoader
  dataset: ${dataset}
  batch_size: ${schedule.micro_batch_size}
  num_workers: 4
  collate_fn:
    _target_: speech_lm.datasets.hubert_vq.HubertVQCollator

optimizer:
  _target_: torch.optim.AdamW
  lr: 3e-4
  weight_decay: 0.1
  betas: [0.9, 0.95]
  eps: 1e-5

scheduler:
  _target_: torch.optim.lr_scheduler.LambdaLR
  lr_lambda:
    _target_: speech_lm.scheduler.get_cosine_schedule_with_warmup_lr_lambda
    _partial_: true
    num_warmup_steps: 1000
    num_training_steps: ${schedule.max_steps}
