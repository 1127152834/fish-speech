paths:
  run_dir: results/whisper-vq
  checkpoint_dir: ${paths.run_dir}/checkpoints

hydra:
  run:
    dir: ${paths.run_dir}

trainer:
  _target_: lightning.fabric.Fabric
  accelerator: gpu
  strategy: 
    _target_: lightning.fabric.strategies.DDPStrategy
    static_graph: true

  devices: auto
  precision: bf16-mixed
  loggers:
    _target_: pytorch_lightning.loggers.TensorBoardLogger
    save_dir: ${paths.run_dir}
    name: tensorboard
    version: null

model:
  _target_: fish_speech.models.whisper_vq.WhisperVQ
  model_name_or_path: "openai/whisper-medium"

  # Quantization
  codebook_dim: 32
  codebook_size: 4096
  codebook_decay: 0.9
  threshold_ema_dead_code: 0
  use_cosine_similarity: true
  downsample: true

  # Attention
  post_attention_depth: 2

schedule:
  batch_size: 64
  micro_batch_size: 32
  max_steps: 10000
  save_interval: 2000
  gradient_accumulation_steps: "${eval: ${schedule.batch_size} // ${schedule.micro_batch_size}}"
  clip_grad_norm: 2.0
  log_interval: 50
  eval_interval: 2000

train_dataloader:
  _target_: torch.utils.data.DataLoader
  dataset:
    _target_: fish_speech.datasets.whisper_vq.WhisperVQDataset
    filelist: filelists/whisper-vq.train.filelist
  batch_size: ${schedule.micro_batch_size}
  num_workers: 16
  prefetch_factor: 4
  pin_memory: true
  persistent_workers: true
  shuffle: true
  collate_fn:
    _target_: fish_speech.datasets.whisper_vq.WhisperVQCollator

valid_dataloader:
  _target_: torch.utils.data.DataLoader
  dataset:
    _target_: fish_speech.datasets.whisper_vq.WhisperVQDataset
    filelist: filelists/whisper-vq.test.filelist
  batch_size: 16
  num_workers: 8
  prefetch_factor: 4
  pin_memory: true
  shuffle: false
  collate_fn:
    _target_: fish_speech.datasets.whisper_vq.WhisperVQCollator

optimizer:
  _target_: torch.optim.AdamW
  lr: 3e-4
  weight_decay: 0.1
  betas: [0.9, 0.95]
  eps: 1e-5

scheduler:
  _target_: torch.optim.lr_scheduler.LambdaLR
  lr_lambda:
    _target_: fish_speech.scheduler.get_cosine_schedule_with_warmup_lr_lambda
    _partial_: true
    num_warmup_steps: 1000
    num_training_steps: ${schedule.max_steps}
