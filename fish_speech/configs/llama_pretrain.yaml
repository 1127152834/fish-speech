defaults:
  - base
  - _self_

project: llama_pretrain

# Say we want a 3 trillion seen token schedule
# 3e12 / 1024 / 512 / 8 = 715255
# But we use a 100k steps schedule here to save time
# This is a 400 billion seen token schedule:
# 1024 * 512 * 8 * 100000 = 419_430_400_000

# Lightning Trainer
trainer:
  accumulate_grad_batches: 64
  gradient_clip_val: 1.0
  gradient_clip_algorithm: 'norm'
  num_nodes: 1
  limit_val_batches: 100 # 100 batches for validation

# Dataset Configuration
tokenizer:
  _target_: transformers.AutoTokenizer.from_pretrained
  pretrained_model_name_or_path: fishaudio/speech-lm-300m
  revision: init

# Dataset Configuration
dataset:
  _target_: fish_speech.datasets.text.InterleaveDataset
  datasets:
    - _target_: fish_speech.datasets.text.StreamTextDataset
      prefix: 'en/'
    - _target_: fish_speech.datasets.text.StreamTextDataset
      prefix: 'zh/'
    - _target_: fish_speech.datasets.text.StreamTextDataset
      prefix: 'ja/'
  probabilities: [0.4, 0.3, 0.3]
  seed: 42

data:
  _target_: fish_speech.datasets.text.TextDataModule
  train_dataset: ${dataset}
  val_dataset: ${dataset}
  num_workers: 4
  batch_size: 8
  tokenizer: ${tokenizer}

# Model Configuration
model:
  _target_: fish_speech.models.text2semantic.TextToSemantic

  model:
    _target_: transformers.AutoModelForCausalLM.from_pretrained
    pretrained_model_name_or_path: fishaudio/speech-lm-300m
    revision: init

  optimizer:
    _target_: torch.optim.AdamW
    _partial_: true
    lr: 3e-4
    weight_decay: 0.1
    betas: [0.9, 0.95]
    eps: 1e-5

  lr_scheduler:
    _target_: torch.optim.lr_scheduler.LambdaLR
    _partial_: true
    lr_lambda:
      _target_: fish_speech.scheduler.get_cosine_schedule_with_warmup_lr_lambda
      _partial_: true
      num_warmup_steps: 2000
      num_training_steps: ${trainer.max_steps}
      final_lr_ratio: 0.1
